{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Se busca minimizar la funcion $2x^2+2xy+\\frac{3}{2}y^2-x+y+1$, emplearemos metodo del gradiente y exacta, para ello\n",
        "realizaremos 2 funciones una para evaluar la funcion y otra para el gradiente\n"
      ],
      "metadata": {
        "id": "zznDMSZff01T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "NnnwGUDUi17u"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(X):\n",
        " return 2*X[0]**2+3/2*X[1]**2+2*X[0]*X[1]-X[0]+X[1]+1"
      ],
      "metadata": {
        "id": "xmqK_p73n8AJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HmKdn3vrgDev"
      },
      "outputs": [],
      "source": [
        "def grad(X):\n",
        " return np.array([4*X[0]+2*X[1]-1,2*X[0]+3*X[1]+1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metodo del gradiente**  \n",
        "nuestro objetivo es aproximar el valor  de $λ$ para lo cual lo iremos multiplicando por 0.5 hasta que cumpla las condiciones de wolfe y armijo"
      ],
      "metadata": {
        "id": "uJHRudlkgo2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alp, bet, k, ep, N=10**-4,10**-1,0,10**-3,50\n",
        "Xk=[3,-2] #punto inicial\n",
        "while(np.linalg.norm(grad(Xk)) >= ep and k <= N): #condciones de paro\n",
        "    dk = -grad(Xk) #metodo del gradiente\n",
        "    lam = 1 #propuesta de lambda\n",
        "    while(f(Xk + lam*dk) > f(Xk) + alp*lam*np.dot(grad(Xk), dk) or\n",
        "           np.dot(grad(Xk + lam*dk), dk) < bet*np.dot(grad(Xk), dk)):#el ciclo continuará mientras se culpa la negacion de las condiciones de wolfe\n",
        "        lam = 0.5*lam #se reasigna el valor de lamda hasta obtener una aproximacion que satifaga las condiciones de wolfe y armijo\n",
        "\n",
        "    Xk=Xk-lam*grad(Xk)\n",
        "    k+=1\n",
        "    print('it:',k,'lamda=',lam,'norma=',np.linalg.norm(grad(Xk)),'X=',Xk,'f(x)=',f(Xk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BynIOaP6i7V3",
        "outputId": "fc956f15-2e95-4611-df9b-6353a8018820"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it: 1 lamda= 0.25 norma= 3.2882366094914763 X= [ 1.25 -2.25] f(x)= 2.59375\n",
            "it: 2 lamda= 0.5 norma= 4.310234912391667 X= [ 1.5   -0.625] f(x)= 2.0859375\n",
            "it: 3 lamda= 0.25 norma= 1.7130587592082183 X= [ 0.5625  -1.15625] f(x)= 0.61865234375\n",
            "it: 4 lamda= 0.25 norma= 0.6996877791424186 X= [ 0.828125  -0.8203125] f(x)= 0.373870849609375\n",
            "it: 5 lamda= 0.25 norma= 0.3032631470801441 X= [ 0.66015625 -0.86914062] f(x)= 0.32788658142089844\n",
            "it: 6 lamda= 0.25 norma= 0.1453774911891728 X= [ 0.68457031 -0.79736328] f(x)= 0.31731927394866943\n",
            "it: 7 lamda= 0.5 norma= 0.1788597946155405 X= [ 0.61279297 -0.78588867] f(x)= 0.3156062066555023\n",
            "it: 8 lamda= 0.25 norma= 0.07145371430948781 X= [ 0.64294434 -0.75286865] f(x)= 0.31305339001119137\n",
            "it: 9 lamda= 0.25 norma= 0.0295379771096505 X= [ 0.62643433 -0.75968933] f(x)= 0.31261714396532625\n",
            "it: 10 lamda= 0.25 norma= 0.013102470036576669 X= [ 0.62984467 -0.7531395 ] f(x)= 0.31253130660479655\n",
            "it: 11 lamda= 0.25 norma= 0.006483538760797743 X= [ 0.62656975 -0.75320721] f(x)= 0.31251028846736517\n",
            "it: 12 lamda= 0.5 norma= 0.007429169521871222 X= [ 0.62663746 -0.74996614] f(x)= 0.31250547513570837\n",
            "it: 13 lamda= 0.25 norma= 0.0029874187344444644 X= [ 0.62498307 -0.75081027] f(x)= 0.3125010128003609\n",
            "it: 14 lamda= 0.25 norma= 0.0012532327315159682 X= [ 0.62540513 -0.7501941 ] f(x)= 0.31250022750422124\n",
            "it: 15 lamda= 0.25 norma= 0.0005706717433854275 X= [ 0.62509705 -0.75025109] f(x)= 0.3125000646711018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**exacta**  \n",
        "reescribimos la funcion de manera vectorial ($f(x)=X^TAX-b^T+c$)  \n",
        "de manera que:  \n",
        "$A=\\begin{bmatrix}\n",
        "    4 & 2 \\\\\n",
        "    2 & 3 \\\\\n",
        "\\end{bmatrix}$  \n",
        "$b=\\begin{bmatrix}\n",
        "    1  \\\\\n",
        "    -1 \\\\\n",
        "\\end{bmatrix}$  \n",
        "c=1  \n",
        "Tenemos que para calcular exactamente λ:  \n",
        "λ=$\\frac{gk^Tgk}{gk^TAgk}$ donde gk= grad(X)\n"
      ],
      "metadata": {
        "id": "5YbcUNDl3BM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A=np.array([[4,2],[2,3]]) #matriz A"
      ],
      "metadata": {
        "id": "PvcFf6za3HxQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alp, bet, k, ep, N=10**-4,10**-1,0,10**-3,50\n",
        "Xk=[3,-2]\n",
        "while(np.linalg.norm(grad(Xk))>=ep and k<=N):\n",
        "  gk=grad(Xk)\n",
        "  lam=np.dot(gk,gk)/np.dot(np.dot(gk,A),gk)#lambda exacto\n",
        "  Xk=Xk-lam*gk\n",
        "  k+=1\n",
        "  print('it:',k,'lamda=',lam,'norma=',np.linalg.norm(grad(Xk)),'X=',Xk,'f(x)=',f(Xk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy0VRzHX6KZ_",
        "outputId": "c243e103-f0bf-493f-d5f1-0aec4fc4ed11"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it: 1 lamda= 0.22026431718061673 norma= 2.7723569835067288 X= [ 1.45814978 -2.22026432] f(x)= 2.4933920704845813\n",
            "it: 2 lamda= 0.4065040650406505 norma= 2.0060144026999907 X= [ 1.29877153 -1.1046166 ] f(x)= 0.931204419265369\n",
            "it: 3 lamda= 0.22026431718061668 norma= 0.786499038944048 X= [ 0.86135899 -1.1671041 ] f(x)= 0.4880222844812502\n",
            "it: 4 lamda= 0.40650406504065084 norma= 0.5690928005367504 X= [ 0.81614445 -0.85060234] f(x)= 0.3622944921519995\n",
            "it: 5 lamda= 0.22026431718061665 norma= 0.22312449007828547 X= [ 0.69205346 -0.86832963] f(x)= 0.3266263626781275\n",
            "it: 6 lamda= 0.4065040650406501 norma= 0.16144780176396267 X= [ 0.6792264  -0.77854021] f(x)= 0.31650755412676645\n",
            "it: 7 lamda= 0.22026431718061662 norma= 0.06329891787221399 X= [ 0.64402261 -0.78356932] f(x)= 0.31363691616482625\n",
            "it: 8 lamda= 0.40650406504065334 norma= 0.04580165602135795 X= [ 0.64038366 -0.75809667] f(x)= 0.31282253547299843\n",
            "it: 9 lamda= 0.22026431718061676 norma= 0.017957477470928962 X= [ 0.63039659 -0.75952339] f(x)= 0.31259150114543244\n",
            "it: 10 lamda= 0.40650406504064 norma= 0.012993621909859065 X= [ 0.62936424 -0.75229697] f(x)= 0.3125259582598391\n",
            "it: 11 lamda= 0.22026431718062134 norma= 0.005094415638667517 X= [ 0.62653098 -0.75270172] f(x)= 0.312507364183811\n",
            "it: 12 lamda= 0.4065040650406069 norma= 0.0036862031857025097 X= [ 0.62623811 -0.75065163] f(x)= 0.3125020891694411\n",
            "it: 13 lamda= 0.22026431718062556 norma= 0.0014452514692843154 X= [ 0.62543433 -0.75076646] f(x)= 0.31250059268332586\n",
            "it: 14 lamda= 0.40650406504059916 norma= 0.0010457510631405617 X= [ 0.62535124 -0.75018486] f(x)= 0.31250016814027526\n",
            "it: 15 lamda= 0.22026431718069456 norma= 0.0004100081260772395 X= [ 0.62512322 -0.75021744] f(x)= 0.31250004770026574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De  manera que la busqueda inexacta atraves del metodo del gradiente almenos por esta vez resulto ser igual de eficaz"
      ],
      "metadata": {
        "id": "zbo8xNRwkV9D"
      }
    }
  ]
}